\RequirePackage[l2tabu, orthodox]{nag}
\documentclass[a4paper, twocolumn]{article}
\usepackage[english]{isodate}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[pdftex,
            pdftitle={Behaviour Tree Evolution by Genetic Programming},
            pdfauthor={Martin Estgren and Erik S. V. Jansson},
            pdfsubject={Artificial Intelligence - Genetic Programming},
            pdfkeywords={artificial intelligence, genetic programming,
                         behaviour trees, a-star, shooter}]{hyperref}
\usepackage{bm}
\usepackage{caption}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{algorithmic}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{courier}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[capitalize, noabbrev]{cleveref}
\usepackage[activate={true, nocompatibility}, final,
            tracking=true, kerning=true, spacing=true,
            factor=1100, stretch=10, shrink=10]{microtype}

\newcommand\blfootnote[1]{%
    \begingroup
        \renewcommand\thefootnote{}\footnote{#1}%
        \addtocounter{footnote}{-1}%
    \endgroup
}

\DeclareCaptionFormat{modifiedlst}{\rule{\textwidth}{0.85pt}\\[-2.9pt]#1#2#3}
\captionsetup[lstlisting]{format =  modifiedlst,
labelfont=bf,singlelinecheck=off,labelsep=space}
\lstset{basicstyle=\footnotesize\ttfamily,
        breakatwhitespace = false,
        breaklines = true,
        keepspaces = true,
        language = Java,
        showspaces = false,
        showstringspaces = false,
        frame = tb,
        numbers = left,
        numbersep = 5pt,
        xleftmargin = 16pt,
        framexleftmargin = 16pt,
        belowskip = \bigskipamount,
        aboveskip = \bigskipamount,
        escapeinside={<@}{@>}}

\title{\textbf{Behaviour Tree Evolution by Genetic Programming}\\
       \Large{\emph{-- Learning Novel Bot Behaviours in a 2D Top-Down Arena Shooter --}}}
% Added course information using date-hijacking 
\date{
TNM095 -- Artificial Intelligence for Interactive Media\endgraf
at Linköping University, \today\endgraf\bigskip --
\href{https://github.com/sci10n/Quake2D}{\textbf{Repository Link}}
$\bullet$
\href{https://vimeo.com/242262802}{\textbf{Video Link}} --
}
\author{{\textbf{Martin Estgren}} \;\;\;\;\;\;\;\;\;\, {\href{mailto:mares480@student.liu.se}
                                                       {\texttt{<mares480@student.liu.se>}}} \\
        {\textbf{Erik S. V. Jansson}} \;\;\;\;         {\href{mailto:erija578@student.liu.se}
                                                       {\texttt{<erija578@student.liu.se>}}} \\~\\
        \vspace{-5.0ex}}

\begin{document}
    \maketitle
    

    \section*{Abstract}

    Behaviour trees are a popular model for representing the decision-making and plan execution process for NPCs in video games. These are built by hand, and often require expertise to craft into interesting and intelligent behaviours. These types of behaviour trees usually don't adapt well to new environments, and need many tailor-made trees for each situation.

    In this paper we demonstrate how to generate the trees by using genetic programming; allowing us to essentially evolve novel behaviours automatically in our testbed. Instead of specifying low-level actions and conditions, we use high-level definitions. This leads to faster fitness convergence, and allows us to skip the bloat control and tree pruning steps used by other similar behaviour tree evolution methods. Results show that the evolved trees reliably defeats our hand-crafted behaviours by the \(25^{th}\) generation.

    \vspace{1em}

    \begingroup
    \def\addvspace#1{}
    \tableofcontents
    \endgroup

    \section{Introduction} \label{sec:introduction}

    In interactive media such as video games, there is often a need for simulating seemingly complex agent behaviours in (soft) real-time. In a modern fast-paced computer game for example, the render, physics and artificial intelligence updates all need to complete in a total of less than 16ms to provide a half-decent experience. These resource constraint have spawned some very clever techniques to enable complex behaviours for autonomous agents while minimizing their required computational time.

    These techniques often require the bot behaviours to be manually defined, and in some cases, are quite hard to extend when new behaviours are needed later in the game development process. This has been shown to be true by industry researchers, such as \emph{Dawe et al}~\cite{dawe2014overview}, regarding the \emph{finite state machine architecture} (which we talk more about later).

    In this project we have explored how one of the more widely used techniques, \emph{behaviour trees}, can be extended to allow for not only hand-crafted complex behaviours, but for organic behaviours, tailored to the specific game domain by using a \emph{reinforcement learning} approach. More specifically, we evolve the behaviour trees by using \emph{genetic programming}. It's by no means a novel approach, but there still has been relatively little research in the area. In some previous work, such as in \emph{Ögren et al.}~\cite{colledanchise2015learning}, this method of pairing behaviour trees with genetic programming is called \emph{behaviour tree evolution (BTE)}.

    Our work was initially inspired by and based on the research of \emph{Ögren et al.}~\cite{colledanchise2015learning}, but our approaches diverged later on in the project for a couple of reasons. First of all, the domain in \emph{Ögren et al's} work allows for relatively simplistic agent actions and conditions (it's based on the \emph{MarioAI} project), which isn't suitable for our domain since the actions and conditions our agent needs are a lot more complex. Therefore, it wasn't viable to have \emph{low-level actions} with high granularity (e.g. move left, right or jump), but we instead needed \emph{high-level actions} (e.g. move to the enemy), otherwise the behaviour tree would quickly grow too deep and become unmanageable. The second reason we diverged from \emph{Ögren et al's.} work was because their method needs \emph{tree pruning} and \emph{bloat control} to remain viable, and we wanted our method to remain simple to implement in code.

    Therefore, our method differs from \emph{Ögren et al.}~\cite{colledanchise2015learning} in the aspect that we specify actions and conditions using a high-level specification instead of a low-level one. This allows us to skip the pruning and bloat control steps in \emph{Ögren et al.} since the behaviour tree tends to converge faster and also not grow too deep, which gives us a somewhat simpler implementation as well. Another difference is that our selection, crossover and mutation steps in the GP are somewhat different than in \emph{Ögren et al.}, and that we also didn't need to use simulated annealing as a pre-process step to get a good generation diversity.

    Alongside the proposed method in \cref{sec:proposed_approach}, a testbed consisting of a top-down arena shooter was built. It features a sufficiently complex environment to support interesting behaviours by the enemy bot. We describe the features of our testbed in \cref{sec:results_and_screenshots}.

    This report starts off in \cref{sec:background_theory} by giving a brief overview of the relevant techniques and related work necessary to follow our reasoning and findings. In \cref{sec:implementation_details} we describe the details needed to implement these techniques in practice, along with the testbed architecture we have used. This gives the reader information about its potential pit-falls. We follow this by showing in \cref{sec:results_and_screenshots} the testbed, and give some generated offspring generated by our method along with measurements of its efficiency. Finally, we reflect and give downsides of our method, and present directions for future work in \cref{sec:discussion_and_outlook}.

    \subsection{Proposed Approach} \label{sec:proposed_approach}

    The primary approach during this project is to examine how \textit{genetic programming} applied to the generation \emph{behaviour trees} can serve as a substitute for behaviours hand-crafted by game designers. 

    \section{Background Theory} \label{sec:background_theory}

    In this chapter we briefly describe the theory behind the three techniques used in this project: \emph{behaviour trees}, \emph{path finding} and \emph{genetic programming}. These techniques are only briefly described here to make the report self-contained. However, we also give many references in the bibliography for that readers that want to read these topic more in-depth.

        \subsection{Behaviour Trees} \label{sec:behaviour_trees}

        Defining an accurate \emph{automatic planner} model is often impractical and usually overkill for real-world applications. Especially in games, where very simple models are enough to give the illusion of intelligence. In games, the most well-known \emph{behaviour selection algorithms} are \emph{FSMs}, \emph{GOAPs}, \emph{HTNs} and \emph{BTs} \cite{dawe2014overview}.

        \emph{Finite State Machines (FSMs)} are simple but hard to extend with additional states, resulting in an exponential increase of transitions \cite{dawe2014overview}. While both \emph{GOAP (Goal-Oriented Action Planner)} and \emph{HTN (Hierarchical Task Network)} are powerful models, these don't allow agents to explore new task definitions, and ``only'' provide an agent with novel ways of solving an already defined set of tasks. Planning has seen some use in the games industry. Most notably in the game \textit{F.E.A.R}~\cite{orkin2006three} where adversarial agents used a planning system to combat the player.

        \emph{Behaviour Trees (BTs)} on the other hand, when comparing to FSMs, provide advantages in terms of modularity, reactiveness and scalability. And more importantly, since they are a type of tree, allow for integration with the genetic programming approach. They were first introduced in \emph{Halo 2} \cite{isla2005managing}, but are now applied in other area as well, such as robotics.

        \emph{Behaviour Tree Evolution (BTE)} is a technique which uses some form of genetic programming on BTs. It's a relatively new technique that has been applied to \emph{strategy games} and \emph{platformers}. Some of the previous work is due to \emph{Lim et al.}~\cite{lim2010evolving}, \emph{Colledanchise et al.}~\cite{colledanchise2015learning} and a recent thesis by \emph{Hoff et al.}~\cite{hoff2016evolving}.

        In the \emph{Behaviour Tree Starter Kit (BTSK)}~\cite{champandard2014behaviour} and in \emph{Chris Simpson's Gamasutra article}~\cite{simpson2014behavior}, the BT is a tree-like data structure which describes the decision-making process of the agent. It's evaluated pre-order from the root of the tree, in each logic update tick. There are many other ways to define BTs, but the one above is the most common one, and is very similar to the original, fairly informal one, in the GDC '05 Halo 2 talk by \emph{Damian Isla}~\cite{isla2005managing}.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/behaviour_tree.pdf}
            \caption{Example of Hand-Crafted Behaviour Tree}
            \label{fig:behaviour_tree}
        \end{figure}

        In the coming pages we attempt to summarize the most common types of nodes in a behaviour tree, in particular those found in the \emph{BTSK}~\cite{champandard2014behaviour}. There are two types of nodes, \emph{concrete nodes}, and \emph{abstract nodes}. \emph{Concrete nodes} are \emph{leaf-nodes}, and are suspended at the bottom of the BT. Furthermore, concrete nodes are divided into two types: \emph{action nodes} which allows the agent to change the world state, and \emph{condition nodes} which allow the agent to query the world state. On the other spectrum are \emph{abstract nodes}, which can either be \emph{composite nodes} (which usually deals in the control-flow of the BT, by changing the order in which its children are executed) or \emph{decorator nodes} (which modify existing child nodes).

        Each node has a state attached to it: \emph{RUNNING}, \emph{SUCCEEDED} or \emph{FAILED}. If its \emph{running}, then this means the current action or condition hasn't been completely yet (this in turn also affects the parent node), but will eventually complete. If, once completed, the node \emph{succeeded} or \emph{failed} (according to some pre-defined convention), then the node's state becomes set to that (\emph{FAILED} or \emph{SUCCEEDED} state). We now summarize the common node types:

        \begin{itemize}
            \item{\textbf{Selector (composite node):} evaluates its child nodes from left to right, until \emph{one of the children} is \emph{RUNNING} or has \emph{SUCCEEDED}, and inherits its state. Otherwise, the selector receives a \emph{FAILURE} state.}
            \item{\textbf{Sequence (composite node):} evaluates its child nodes from left to right, until \emph{one of the children} is \emph{RUNNING} or has \emph{FAILED}, and inherits its state. If all children succeed, then the sequence also \emph{SUCCEEDS}, otherwise it \emph{FAILS}.}
            \item{\textbf{Inverter (decorator node):} evalutes its only child node and \emph{inverts the resulting state once it leaves} the \emph{RUNNING} state. i.e. \emph{SUCCEEDED} state becomes a \emph{FAILED} state, and vice-versa.}
            \item{\textbf{Succeeder (decorator node):} regardless of what the child's state is once it has \emph{finished RUNNING}, we return only a \emph{SUCCESS} state.}
            \item{\textbf{Condition (leaf node):} this node queries the world state, and if the query returns a positive answer (e.g. our agent has bullets for his weapon), we return a \emph{SUCCESS}, if it's negative, we give a \emph{FAILURE}. This node is usually instantaneous, and thus can't be in a \emph{RUNNING} state.}
            \item{\textbf{Action (leaf node):} modifies the world state, and returns a \emph{SUCCESS} if the action had the intended side-effect (e.g. the agent is shooting at the enemy), or returns\emph{FAILURE} if it couldn't complete the action for some reason. If we are still performing the action on the world (e.g. aiming the gun), then the node is still \emph{RUNNING}.}

        \end{itemize}

        We find that it's easier to understand behaviour trees with an example. We'll be using the tree in \cref{fig:behaviour_tree} as an example (it was produced within our testbed using a Graphviz library for Java). The \emph{yellow} nodes means the node state is \emph{running}, the \emph{red} nodes have \emph{failed}, and \emph{green} nodes have \emph{succeeded}.

        At a high-level, this BT checks the agent's health and armour, and if it's below a threshold, it will go and pick up the crates which replenish these stats.

        It does this by starting at the root node, \emph{selector}, and executing the left child, the \emph{sequence}. It will execute its left child, which is the \emph{inverter}. At last we reach a leaf-node, the \emph{health condition node}. This will probe the world for the state of the agent, and return \emph{success} because we've found that it's above 50\%. This result will propagate upward to its parent, the inverter, which will set its state to \emph{failed} (inverting the result of the child). We're back at the left sequence, and because one of the children have failed (the inverter), the sequence has \emph{failed} as well, and won't run the right child (gray means the node has never been evaluated). Finally, because the selector's left child has \emph{failed}, we execute the right branch too (remember, at least one child must succeed!). We use the same steps as before, but get stuck in \emph{``pick up armour''} until that action has finished \emph{running} in the world (has succeeded or failed).

        \subsection{Path Finding with A*} \label{sec:path_finding}

	Agent path finding is done using the \emph{A*} graph traversal algorithm proposed by Peter E. Hart, Nils J. Nilsson, and Bertram Raphael ~\cite{hart1968formal} as an extension to \emph{Dijkstra's algorithm} which, given an \emph{admissible heuristic} and non-negative costs, will find the path from a node \(n_0\) to a node \(n_g\), with the lowest cost.
	
    Calculating the expected cost of a path \(n_0 \rightarrow n_k \rightarrow n_g\), from \(n_0\) to \(n_g\) via \(n_k\) can be done with the following path finding cost function:
	\begin{equation*}
		f(n_k) = g(n_k) + h(n_k)
	\end{equation*}	
	where \(g(n_k)\) is the true cost from \(n_0\) to \(n_k\) and \(h(n_k)\) is a heuristic approximation of the cost from \(n_k\) to \(n_g\). This allows us to explore the potentially most optimal paths first before considering the others.

        In this project, the \emph{Euclidean distance} is used as the heuristic \(h(n_k)\). This, since the path finding is performed in a search-space where each node is defined as a point in the testbed's 2-D world-space.

    Additionally, compared to regular \emph{A*}, the path-finder utilizes an \emph{influence map} to help the agent seem more ``aware'' of it's surrounding, even if the selected path is optimal given the heuristic function. Our \emph{influence map}, like the ones shown in \emph{Dave Mark}~\cite{mark2015modular}, take into account the visibility of each node in relation to the adversarial game agents' line-of-sight, and therefore updates the expected cost as:
	\begin{equation*}
		f(n_k) = g(n_k) + h(n_k) + i(n_k)
	\end{equation*} 
    where \(i(n_k)\) is the cost introduced by the influence map. This gives the the result that nodes within the adversarial agent's line-of-sight have a much higher associated traversal cost, and should be avoided. 
    \begin{minipage}{\linewidth}        	
    \centering
	\begin{minipage}{0.45\linewidth}
	\begin{figure}[H]
        \centering
		\includegraphics[width=\linewidth]{share/bad.eps}
		\caption{optimal path}
		\label{fig:optimal_path}
        \end{figure}
	\end{minipage}
	\hspace{0.00\linewidth}
	\begin{minipage}{0.45\linewidth}
	\begin{figure}[H]
        \centering
		\includegraphics[width=\linewidth]{share/good.eps}
		\caption{tactical path}
		\label{fig:smart_path}
	\end{figure}
	\end{minipage}
\end{minipage}

\vspace{0.5em}

    Above are two example figures: \cref{fig:optimal_path} shows an optimal path using regular \emph{A*}, and the other, \cref{fig:smart_path} shows the optimal path when taking into account the influence map; sneaking behind cover.
	
	\subsection{Genetic Programming} \label{sec:genetic_programming}

    In many scenarios you try to optimize a function: \(f(x_0,x_1,...,x_n)\), where some or all of the inputs are of a discrete nature (ordinal or nominal values). These types of problems are often hard to optimize using techniques developed for continuous variables, such as \emph{gradient ascent/descent}, e.g. in a \emph{perceptron}.

	\emph{Genetic programming} is a technique that was introduced by \emph{Barricelli et al}~\cite{barricelli1954esempi}, as a way to optimize computer programs by encoding the parameters to be optimized as \emph{genetic representations} which could be processed by traditional \emph{evolutionary algorithms}.

	\begin{figure}[H]
        \centering
		\includegraphics[width=0.5\linewidth]{share/Genetic_Program_Tree.png}
        \caption{The \emph{genetic representation} is often done using a tree-like structure in \emph{genetic programming}. Image is taken from: \texttt{\href{https://upload.wikimedia.org/wikipedia/commons/7/77/Genetic\_Program\_Tree.png}{Wikipedia}} (public domain).}
		\label{fig:genetic_representation}
	\end{figure}

	The most general form of the \emph{evolutionary algorithm} can be described in the following steps:
    \begin{enumerate}
        \item Generate an initial population by creating a initial pool \(\mathcal{P}_0\), of individuals with randomized genetic representations (context-specific data).
        \item Evaluate the \emph{fitness} for each individual \(x_i \in \mathcal{P}_t\). This usually requires a domain-specific \emph{fitness function}: \(f(x_i)\rightarrow f_i \in \mathbf{R}\), which will be described in further details later in this chapter.
        \item Remove individuals with poor \textit{low fitness score} and \emph{regenerate} population pool: \(\mathcal{P}_{t+1} = \Phi(\mathcal{P}_t)\).
    \end{enumerate}

    The fitness function for this project is composed of a \emph{linear combination} of desirable behaviours collected during the \emph{fitness evaluation} simulation, such as, amount of \emph{damage dealt} to the adversary, whether the agent with a specific BT \emph{survived}, and how many \emph{weapons the agent has picked-up}. In the implementation section we'll specify \(f_i\) more concretely, for now we just explain the general theory behind genetic programming and how to find \(\mathcal{P}_{t+1}\).

	\emph{Regeneration} of the population pool for the next generation, \(\mathcal{P}_{t+1}\), is done using the three functions:

    \vspace{1em}

    \textbf{Selection} ~ \(\Phi_s(\mathcal{P}_t, \mathbf{R}) \rightarrow x_i \in \mathcal{P}_t\) ~ individuals with high \emph{fitness score} in the new population \(\mathcal{P}_{t+1}\). In this project, \emph{selection} is done proportionally to the \emph{fitness score} for each individual. Each individual \(x_i\) has a probability \(P(x_i|f_i)\) to be selected for the next generation \(\mathcal{P}_{t+1}\) according to some random process $\mathbf{R}$.
    \begin{equation*}
        P(x_i|f_i) = \frac{f_i}{\sum_{j = 1}^{N}f_j} \; \; \text{where} \; \; N = |\mathcal{P}_t|
    \end{equation*}

    \textbf{Crossover} ~ \(\Phi_c(x_i,x_j,\mathbf{R}) \rightarrow \hat{x}_i,\hat{x}_j\) ~ individuals from the new population which have been crossed over, in the case of \emph{genetic programming}, \emph{crossover} is done by swapping random \emph{sub-trees} in each BT. All sub-trees have an equal probability of being selected to crossover, creating new individuals in \(\mathcal{P}_{t+1}\). $\mathbf{R}$ represents the random seed for the crossover process.

    \vspace{1em}

    \textbf{Mutation} ~ \(\Phi_m(x_i,\mathbf{R}) \rightarrow \hat{x}_i\) ~ a subset of the new population, but slightly tweaked with a random parameter (based on the random seed $\mathbf{R}$) or by adding/removing random child nodes. If no mutation is done, then the genetic programming might converge to some local optima because of monoculture, and never reach a global optimum.

    \vspace{1em}

    Once these steps have been performed, a new generation of individuals have been produced, which can then be used to repeat the optimization process until we converge to a population with behaviours close the global maximum of the fitness function \(f_i\).


    \section{Implementation Details} \label{sec:implementation_details}

        After presenting these theoretical methods we'll now describe the additional changes necessary to make these work in practice (as per our implementation).

        Both the top-down shooter game (target \emph{testbed}) and our \emph{behaviour tree evolution} (the \emph{technique}) are written in \emph{Java} and use \emph{libGDX} for graphics/audio. This has allowed us to quickly produce a prototype, and while the game itself isn't the main topic of this paper, a rough outline can be found in \cref{sec:game_architecture}.

        Every \emph{entity} in the game can be transformed into an intelligent agent with a \texttt{BotInputComponent}, which associates a given \emph{behaviour tree} with a entity. It hooks together with the \emph{path finder} to enable the entity to \emph{traverse \& interact} with the environment. With GP, additional BTs can be generated and used.

        \subsection{Game Architecture} \label{sec:game_architecture}

        Given the very limited implementation time of this project, the architecture of the game was designed to allow for fast prototyping iterations, and isn't necessarily based on strong and scalable software design philosophies. The structural design is based on the \textit{entity component system} design, which has seen high use in the contemporary games industry and is being used in high-profile projects such as the \textit{Unity} game engine. The primary reference implementation is taken from the book \textit{Game Programming Patterns} written by \textit{R. Nystrom}~\cite{nystrom2014game} remixed with the component system from \textit{M. Boström}s' \textit{Speed Coding Zelda}\footnote{\url{https://github.com/MilleBo/SpeedCodingZelda}}. The resulting implementation can be seen in the source code in the package: \texttt{se.sciion.quake2d.level} and its sub-folders.

        An \textit{entity component system (E-C-S)}, as the name might suggest, consists of three interacting class types. The \textit{entity}, the \textit{component}, and the \textit{system}.

        The \emph{entity} is defined as a class consisting of a series of \emph{components}, together with functions for querying information about the entities' components. This allows the entities' components to query their parent (the entity itself) regarding other components within this entity. Meaning, components that heavily depend on each other have high coupling, while independent components don't have any coupling. This gives us a very modular design, and gives the expressive power to easily add new components to an entity on-the-fly. There are several other ways to solve the problem of system-system communication, like message-passing, but we've decided to take the ``easy way out'' and just pass references around to the relevant objects.

        In \cref{fig:entity_component_system} are the three different types of entities in our game: \emph{user-controlled players}, \emph{AI-controlled agents} and \emph{health/armour/weapon crate pickups}. As can be seen, adding functionality to each entity is just a matter of adding/swapping a component. The only difference between a \emph{player} and an \emph{agent} is the method of input, \emph{UserInput} vs \emph{BotInput}, which in our opinion perfectly shows the benefits of a E-C-S.

        The \texttt{BotInputComponent} has a behaviour tree which is updated with \texttt{tick}, which executes the motions specified in the \emph{current target path} (basically a stack of locations to visit to reach the \emph{target location}) which were decided by the behaviour tree.
        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.8\linewidth]{share/player_entity.eps}
            \includegraphics[width=0.8\linewidth]{share/agent_entity.eps}
            \includegraphics[width=0.8\linewidth]{share/pickup_entity.eps}
            \caption{List of Components in Entity}
            \label{fig:entity_component_system}
        \end{figure}

        Outside the entity, a set of \emph{systems} are running, which manage inter-entity communication such as \emph{path-finding}, \emph{physics}, and \emph{gameplay}. As we touched upon before, these systems are passed as references to the components that are interested in them, allowing for the components to decide what to do with the data provided by the systems without caring about the system's implementation details.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.7\linewidth]{share/path_finding_stack.png}
            \caption{Visualization of the path finding stack \(\mathcal{F}\). If there are no obstacles in the way (line-of-sight check), the agent takes a straight-line path instead.}
            \label{fig:path_finding_stack}
        \end{figure}

        \twocolumn[{
        \begin{figure}[H]
            \centering
            \includegraphics[width=\textwidth]{share/behaviour_tree_uml.eps}
            \label{fig:behaviour_tree_hierarchy}
        \end{figure}
        }]

        \subsection{Behaviour Trees} \label{sec:behaviour_trees_implementation}

        Inside of the \emph{Java package} \texttt{se.sciion.quake2d.ai.behaviour} (can e found in the source code repository) you'll find the implementation of our BTs. They are specified using inheritance, as in the figure above. In our variant, only the leaves above are non-abstract.

        A behaviour tree is represented as a tree of behaviours where each node contains links to their respective child nodes. The tree is evaluated in a recursive fashion which makes it possible for each behaviour tree to only care about the root-behaviour.

        All nodes are derived from the class \texttt{BehaviourNode}, which provides the framework for each node. Each node is processed for each step (tick) of the simulation though the \emph{tick} function.

        \begin{algorithm}[H]
            \caption{Pseudo-Code for the BT's \emph{Tick} function}
            \label{alg:behaviour_tree_update}
            \begin{algorithmic}
                \REQUIRE{some initial \(State\) to the behaviour tree}
                \IF{\(State \neq Running\)}
                    \STATE \(doBehaviourEnter()\)
                \ENDIF
                \STATE \(State \leftarrow doBehaviourUpdate()\)
                \IF{\(State \neq Running\)}
                    \STATE \(doBehaviourExit()\)
                \ENDIF
            \end{algorithmic}
        \end{algorithm}
        The tree is evaluated recursively by the root-nodes \emph{tick} function being called, which in turn calls the \emph{tick} function in its children.

        The standard non-leaf nodes are implemented in a straight-forward way following \cref{sec:behaviour_trees}. Instead, we continue by explaining the \emph{actions} \& \emph{conditions} we have implemented, and how these communicate with the other systems to be able to query the world.

        We've adopted a fairly simple solution, where the leaf-node is given a reference to systems that it wants to communicate with, and the entity owner.

        For example, if a BT wants a \emph{AttackNearest} node beneath a \emph{Succeeder}, we'd first have to hand over a reference to the \emph{path finder} and a \emph{target tag} to the node's constructor, such as: \emph{AttackNearest(path-finder, tag)}. Only after that are we allowed to execute: \emph{SucceederNode(attack-nearest)}, which we can utilize to construct the next parent layer above it.

        Below is a list of all actions and conditions we've implemented in our game, along with a description:

        \begin{itemize}
            \item{\textbf{AttackNearest(\emph{tag}):} finds the closest entity with \emph{tag} and \emph{path find} to it. Shoot with weapon. \emph{Success} if we were able to fire, \emph{false} otherwise.}
            \item{\textbf{MoveToNearest(\emph{tag}):} finds the close entity with \emph{tag} and \emph{path find} to it. Stops when close. \emph{Success} when we're close, \emph{fails} if no valid path.}
            \item{\textbf{PickUpConsumable(\emph{type}):} finds the closest consumable of \emph{type} (\emph{health}, \emph{armour} and \emph{boost}), \emph{path find} to it and \emph{pick it up}. \emph{Fails} if it couldn't.}
            \item{\textbf{PickUpWeapon(\emph{type}):} finds a close weapon of \emph{type} (\emph{shotgun}, \emph{rifle} \& \emph{sniper}). \emph{Path find} to it \& try to \emph{pick it up}. \emph{Succeeds} when picked up.}
        \end{itemize}
        \begin{itemize}
            \item{\textbf{CheckStatus(\emph{ratio, type}):} checks the entity owner's \emph{status} (\emph{health} or \emph{armour}), and returns \emph{Success} if above the \emph{ratio}, or \emph{fail} if its below.}
            \item{\textbf{CheckWeapon(\emph{type}):} checks entity owner's \emph{inventory} for weapon \emph{type}. \emph{Success} if it gets it.}
            \item{\textbf{CheckDistance(\emph{tag, r})} checks if entity with \emph{tag} is above \emph{r} units away. \emph{Succeeds} query if so.}
        \end{itemize}

        \clearpage

        \subsection{Genetic Programming} \label{sec:genetic_programming_implementation}

        Since the genetic programming has to be done on the trees during runtime, we need a way to dynamically generate trees with varying structure (changing composite nodes) and behaviours (changing leaf nodes). The solution used in this project is to create a set of \textit{prototype behaviour nodes} \(\mathcal{S}\), one for each node type. This set can then be used to create the trees by building them from the root. The prototypes are implemented using the \textit{prototype design pattern} as presented by \textit{R. Nystrom}~\cite{nystrom2014game}, meaning, we have a set of correctly pre-specified instances which are then cloned \& modified to suit our needs.

        \subsubsection*{Tree Generation}

        Each tree starts by first generating a random node \(n \in \mathcal{S}\) as the \emph{tree root}, and if the node is a \textit{composite}, continue recursively until a complete tree is formed.

        All nodes have a \textit{randomization function} which for each node type randomizes it in different ways. \emph{Leaf nodes} have a trivial randomization function which only tweaks parameters such as \emph{thresholds} (e.g. 0.5 or 0.2) and \emph{game tags} (e.g. \emph{player} or \emph{crate}). \emph{Composite nodes} are randomized by picking a random number between 1 and 6 which represents the number of children. Each child is picked randomly from all possible behaviours. Finally, each child is randomized.

        \subsubsection*{Tree Mutation}

        As with \textit{tree randomization}, the \emph{mutation of trees} is done in a recursive manner, where each node mutates itself, and in the case of composite nodes its children. The mutation only differs from the tree randomization in that nodes are only slightly changed with a certain probability, whereas the randomization can completely change all nodes in a tree. Figure \cref{fig:leaf_mutation} and \cref{fig:child_replacement} shows the mutation of a \emph{leaf} and \emph{composite} node, respectively. 

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/leaf_mutation.eps}
            \caption{Mutation for the Leaf Node}
            \label{fig:leaf_mutation}
        \end{figure}

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/child_replacement.eps}
            \caption{Mutation for Composite Node}
            \label{fig:child_replacement}
        \end{figure}

        \subsubsection*{Tree Crossover}

        During \emph{tree crossover}, the tree is flattened to a list. This enables us to \emph{select two random nodes},where each node in the trees has the same probability to be selected, from \emph{two different trees}, and swap them.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/node_swapping.eps}
            \caption{Crossover between two trees.}
            \label{fig:child_crossover}
        \end{figure}

        \subsubsection*{Fitness Function}
        As alluded in the theory section, the fitness function \(f_i\) is defined as an arbitrary linear combination of game related parameters. The full implementation is described in pseudo-code on the next page.
        \vspace{-0.4em}

        \begin{algorithm}[H]
            \caption{Pseudo-Code for the fitness function}
            \label{alg:fitness_function}
            \begin{algorithmic}
                \REQUIRE {$damage$ being the damage given}
                \REQUIRE {$armor$ being the armour picked-up}
                \REQUIRE {$weapon$ to be the weapons picked-up}
                \REQUIRE {$kills$ to be the number of agents killed}
                $Score$ = $5 *damage$ + $2 * armor$ + $50 * weapon$ + $1000 * kills$
                \IF {Individual Won}
                    \RETURN $Score + 500$
                \ELSE
                    \RETURN $Score$
                \ENDIF
            \end{algorithmic}
        \end{algorithm}
        \vspace{-0.6em}

        Since individuals can be tested more than once each generation, the final fitness score is averaged over all test. This allows for a more fair fitness evaluation since only testing each individual once might make some test unfair. For example a bad tree might score very high against an opponent which is broken and can't move.

    \section{Results and Screenshots} \label{sec:results_and_screenshots}

        Below are a couple of in-game screenshots from \emph{Quake 2D}, our testbed for \emph{behaviour tree evolution}. Here we've chosen to show two bots playing against each other in the arena, each of them loaded with a basic hand-made behaviour tree shown in \cref{fig:hand_crafted_behaviour_tree}.

        As can be seen in \cref{fig:behaviour_tree_screenshot}, the environment is composed of \emph{static collidable walls}, \emph{dynamic pick up crates}, \emph{user/agent controlled players}, and \emph{graphical scenery details} based on \emph{tiled textures}. Maps are specified using the \emph{Tiled} editor, which means we can build new environments to test our solution. We've created a serializer for our behaviour trees, that enables us to save and load behaviour trees to disk.

        Additionally, we've developed several tools for showing our agent's behaviour in real-time. As can be seen, \cref{fig:path_finding_screenshot}, we're able to \emph{visualize path finding information} for the intelligent agents. Here the \emph{yellow line} is the \emph{target path} specified by the \emph{path finding} system given a \emph{target location} (shown as a \emph{yellow cross}) by the \emph{behaviour tree}. In this example, the target is the \emph{rifle pickup crate}. You'll see a coloured grid being displayed. This tells us something about the state of the \emph{influence map}, where the \emph{blue and red lines} are grids that each agent is able to shoot towards, while \emph{magenta lines} are locations where both agents are able to shoot towards.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/screenshot.png}
            \caption{Screenshot of a Bot vs Bot in Quake 2-D}
            \label{fig:behaviour_tree_screenshot}
        \end{figure}

        For debugging and analysing agent behaviour, we've implemented a way to visualize BTs, the one in \cref{fig:hand_crafted_behaviour_tree} from the \cref{fig:path_finding_screenshot} situation, in real-time by using a \emph{Graphviz}\footnote{\url{https://github.com/nidi3/graphviz-java}} library. It opens in a separate window and runs in another thread, where we can see instant changes in the BT's nodes' state.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/path_finding_screenshot.png}
            \caption{Screenshot of Path Finding in Quake 2-D}
            \label{fig:path_finding_screenshot}
        \end{figure}

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/hand_crafted_behaviour_tree.pdf}
            \caption{Hand-crafted BT (larger in Appendix~\ref{app:hand_crafted})}
            \label{fig:hand_crafted_behaviour_tree}
        \end{figure}

        We've uploaded a video\footnote{\url{https://vimeo.com/242262802}} of our solution in action.

        Finally, we present the results from our solution. After generating fifty random individuals for \(\mathcal{P}_0\), we applied genetic programming until we had \(\mathcal{P}_{50}\), the resulting pool of \emph{evolved behaviour trees}. The fitness for each individual in each generation was found by having a match against our crafted behaviour tree, and recording match statistics, used to calculate \(f_i\).

        Each individual (BT) has been evaluated for each generation by pitting them against other individuals in the same generation (we call them \emph{peers} below). Therefore two results will be presented, one where the fitness \(f_i\) is evaluated against our \emph{hand-crafted tree} and one against its \emph{peers} in the same generation.

        In \cref{sec:generated_behaviours} we provide two behaviour trees generated with our solution at \(\mathcal{P}_{50}\) when playing against hand-crafted behaviour trees. We then give in \cref{sec:behaviour_fitness} graphs of the evolution of the fitness function per generation, for both the peer and the hand-crafted evaluation methods from \(\mathcal{P}_0\) up to \(\mathcal{P}_{50}\). In \cref{sec:survival_ratio} we measure the effectiveness of the behaviour tree by looking at the amount of individuals in the generation that have survived against our hand-crafted behaviour trees as the number of generation progresses forward.

        \subsection{Generated Behaviours} \label{sec:generated_behaviours}

        Each fitness evaluation was performed on a randomly selected hand-crafted level to prevent the behaviour trees getting overfitted to a specific level. 

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/tree-fitness-4750.png}
            \caption{Generated tree with a very high fitness score (4750). This behaviour tree is 15 levels deep.}
            \label{fig:tree_fitness_4075}
        \end{figure}
        As can be seen in figure \ref{fig:tree_fitness_4075}. This tree is way too big to provide any significant insight, unless interactively evaluated during run time. Therefore we've chosen not to provide a scaled-up variant of it. The fitness score is very close to the limit for the theoretical limit possible for the levels used during evaluation.

        As the number of generations increase, the trees grow very large. This happens since no pruning is performed for the trees, neither as a post-process step nor in the disguise of reduced fitness score.

        While the depth of this tree is unsatisfactory, the performance it achieves against our hard-coded bot is quite good. It is able to consistently beat it on all different map arenas. In fact, it's actually hard for even the authors of this paper to beat it consistently.

        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth]{share/tree-fitness-3780.png}
            \caption{Generated tree with high fitness (3650). This tree on the other hand is only fours levels deep.}
            \label{fig:tree_fitness_3650}
        \end{figure}
        In figure \ref{fig:tree_fitness_3650} smaller tree of the same generation as the tree in figure \ref{fig:tree_fitness_4075} can be observed, it manages to defeat the hard-coded bot on all levels without bloating to unmanageable size. Prior exploration of generated trees indicated that as long as the tree has a \textit{sequence} of \textit{pickup shotgun} and \textit{attack player}, it would reach a fitness of around \(1000\). Because this kind of sub-tree is quite common, it tends to appear in many BT crossovers.

        Since this behaviour tree is a lot smaller, it's quite easy to analyse the expected behaviour in comparison to the one in \cref{fig:tree_fitness_4075}. At the end of this paper we've therefore provided a scaled-up version in Appendix~\ref{app:generated} so the reader can look through it in more detail.

        \twocolumn[{
        \subsection{Behaviour Fitness} \label{sec:behaviour_fitness}
        Looking at the general fitness over generations, we can observe that once the behaviour trees routinely starts defeating its adversary, the mean fitness plateaus and the variance fills the entire range of possible fitness values. When evaluating each tree against random peers, we see something similar but it converges with a slower rate. One possible hypothesis of why this happens is that the amount of ``junk'' behaviours that get by in \cref{fig:fitness-against-random} at the start are all low-performing BTs. Therefore, there is no real competition at the start until one of them gets a good mutation or crossover. After that, this better BT will very likely pass on its genes onto the next generation and generate a lot better BTs than before. In our hard-coded behaviour tree, this happens quickly because the competition at the start is already quite high.
        \vspace{2em}
        \begin{figure}[H]
            \centering
            \begin{minipage}{\textwidth}  
                \includegraphics[width=\textwidth]{share/result-againts-bot.png}
                \caption{Fitness over generations when evaluating against a hard-coded bot. Higher score is better. The solid bar represents 50\% of the fitness variance. The bar in each box represents the mean value and isolated points represents significant outliers.}
                \label{fig:fitness-against-bot}
            \end{minipage}
            \begin{minipage}{\textwidth}  
                \includegraphics[width=\textwidth]{share/result-random.png}
                \caption{Fitness over generations when evaluating against 2 random peers. Higher score is better. The solid bar represents 50\% of the fitness variance. The bar in each box represents the mean value and isolated points represents significant outliers.}
                \label{fig:fitness-against-random}
            \end{minipage}
        \end{figure}

}]

    \twocolumn[{
        \subsection{Survival Ratio} \label{sec:survival_ratio}
        In the ideal case, the survival rate follows as a consequence of the fitness score. In the two figures below we can see the same trend as in the fitness, the higher the fitness, the higher the survival rate. If this wasn't the case, our fitness evaluation wouldn't provide a good metric for evaluating combat performance. When playing against the hard-coded behaviour tree, it takes around 25 generations until 70\% of the individuals in the population are victorious against it. These results then seem to vary between 60\% and 90\% win-ratio in each generation. On the other hand, when playing against itself, it takes around 28 generations until the same can be said about it. However, it's actually able to achieve a high maximum win-ratio, and has less variation between each generation, usually staying between 76\% to 95\% win-ratio.

        \vspace{2em}
        \begin{figure}[H]
            \centering
            \begin{minipage}{\textwidth}  
                \includegraphics[width=\textwidth]{share/result-victories-against-bot.png}
                \caption{Victories for each generation when evaluating against a hard-coded bot. Higher value is better.}
                \label{fig:victory-against-bot}
            \end{minipage}
            \begin{minipage}{\textwidth}  
                \includegraphics[width=\textwidth]{share/result-victories-random.png}
                \caption{Victories for each generations when evaluating against 2 random peers. Higher value is better.}
                \label{fig:victory-against-random}
            \end{minipage}
        \end{figure}

        }]

        \clearpage

    \section{Discussion and Outlook} \label{sec:discussion_and_outlook}

    There are still some areas in the project which merit further analysis. We'll start with the results section, and see what we can extrapolate from the fitness and survival scores. Afterwards, we look at some limitations of our solution, compare it to related work, and then end with some future improvements.

    \subsection*{Convergence Rate}

    As can be seen in the plots in the result section, the evolutionary algorithm converges to a high game proficiency in just a couple of generations, and manages to stay that throughout the evolution process.

    The fact that it manages to do this may reflect poorly on our skills as game designers. Since one expects that the more complex behaviours that are required to proficiently play the game, the more generations it will take until it produces such well-tuned behaviour trees. As we mentioned before, all it takes to beat our hand-crafted behaviour trees is to play very aggressively. And the evolved trees which are generated clearly reflect this fact.

    It can be viewed as the fitness function, tree mutation, and pool of possible actions being well tuned to the specific requirements of this project. Which interpretation is correct is left to the reader.

    A possible explanation to the different convergence rates when evaluating against a hard-coded tree or adversarial peers is the stochastic nature of both level selection and the randomized adversary. The adversarial peer should be much closer in performance compared to the hard-coded tree. Once the trees which can defeat the hard-coded bot have been generated, additional fitness can only be reached by overfitting to the given level. The trees with peer evaluation on the other hand, can't find a solution on how to reliably defeat the enemy. The adversary co-evolves with them which makes it very hard to find a ``holy-bullet'' behaviour since almost all individuals in the next generations will have it.

    When tested against a human player the evolved behaviours becomes very hard to defeat. Even for behaviour trees with a fitness score of around 3000. One of the reasons could have to do with the agent's superhuman accuracy when shooting, but it has to do with how greedy the agent is when it comes to pickups, sometimes it can play really safe, and keep refilling it's health and sometimes denying pickups!

    \subsection*{Limitations}

    Since we decided to have high level actions, the tree's behaviour often appears clunky compared to a human player. As an example, two bots might decide to walk to the same health pickup when both have a very low health instead of just shooting at each other. Current fitness evaluation is bound to the render thread, which severely limits the number of evaluations that can be done during one second, the amount of actions, object types, and levels have been fairly limited in this project to produce faster results and ease debugging. This leads to the BTs having a limited repertoire of behaviours.

    \vspace{-0.5em}

    \subsection*{Related Work}

    As we mentioned before, our genetic programming solution was initially based on the work by \emph{Colledanchise et al.}~\cite{colledanchise2015learning}. The solution proposed in the paper did not pan out in our case since they use an selection heuristic during the mutation phase, allowing for higher fitness increase over fewer generations. The paper used primitive actions which would not scale to the type of game used in this project without \emph{very} large behaviour trees. With this said, it seems like low-level actions might produce more ``clutch'' behaviours in this kind of fast-paced games. 

    Also, even though the behaviour tree in \cref{fig:tree_fitness_4075} is not unusable in practice, it still became deep. The bloat control and pruning steps in \emph{{\"O}gren et al.}~\cite{colledanchise2015learning} might not have been such a bad idea. However, our solution is algorithmically simple, and gives acceptable results. Thus, we recommend starting simple, and then follow \emph{{\"O}gren et al's.}~\cite{colledanchise2015learning} steps when needed.

    \vspace{-0.5em}

    \subsection*{Future Work}

    Some improvements are to extract the behaviour tree and genetic programming parts as an independent library and to use heuristics during the creation of each new generation. To add additional behaviours and mechanics to the project and have it compete with itself instead of a hard-coded adversary, which has been tested to some extent during the project but not enough to yield interesting results. It would be good to run fitness evaluation in headless mode and become physics independent.

    If you are interested in our solution, feel free to look at our code and apply some of the additions. \footnote{Repository: \url{https://github.com/sci10n/Quake2D}}

    \clearpage

    \nocite{*} % Include all.
    \bibliographystyle{abbrv}
    \bibliography{report}

    \onecolumn
    \clearpage

    \pagenumbering{gobble}

    \appendix
    % Remove the appendix titles from the index page.
    \addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

    \section{Appendix: Our Hand-Crafted Behaviour Tree} \label{app:hand_crafted}

    \begin{figure}[H]
        \centering
        \includegraphics[angle=90,height=0.92\textheight]{share/hand_crafted_behaviour_tree.pdf}
    \end{figure}

    \section{Appendix: A Generated Behaviour Tree} \label{app:generated}

    \begin{figure}[H]
        \centering
        \includegraphics[angle=90,height=0.92\textheight]{share/tree-fitness-3780.png}
    \end{figure}

\end{document}
